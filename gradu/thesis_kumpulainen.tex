\documentclass[utf8,english]{gradu3}

\usepackage{graphicx} % for including pictures

\usepackage{amsmath} % useful for math (optional)

\usepackage{booktabs} % good for beautiful tables

\usepackage{csquotes} % ensure proper formatting

\usepackage[authordate,backend=biber,noibid]{biblatex-chicago}

% NOTE: This must be the last \usepackage in the whole document!
\usepackage[bookmarksopen,bookmarksnumbered,linktocpage]{hyperref}

\addbibresource{thesisReferences.bib} % The file name of your bibliography database

\begin{document}

\title{Artificial General Intelligence - a systematic mapping study}
\translatedtitle{Yleistekoäly - systemaattinen kirjallisuuskartoitus}
\studyline{Mathematical Information Technology}
\avainsanat{%
  Pro Gradu,tutkielma, AI, tekoäly}
\keywords{Master's Theses, AGI, AI, artificial intelligence, systematic 
literature mapping, mapping study}
\tiivistelma{%
  
  Tässä suunnitelmassa käydään läpi pro gradu -tutkielman mahdollista aihetta 
  ja tutkimustapaa. TODO: translate when abstract done
}
\abstract{%
  In this thesis, a systematic mapping study is performed on the field of 
  artificial general intelligence. The goal of the study is to gain insight 
  about the recent developments in the study field. This includes the focus 
  points of the current reserch, possible research gaps, and how the research 
  itself is conducted. TODO: more accurate, proper abstract
}

\author{Samu Kumpulainen}
\contactinformation{\texttt{samu.p.kumpulainen@student.jyu.fi}}
% use a separate \author command for each author, if there is more than one
\supervisor{Vagan Terziyan}
% use a separate \supervisor command for each supervisor, if there
% is more than one

\maketitle

\mainmatter

%Remember to use chapters in the thesis itself
\chapter{Introduction}
The thesis will be a systematic research mapping on the field of 
Artificial General Intelligence (AGI). The goal of the thesis is to identify 
the themes and subfields of AGI research in recent years, what is being 
researched recently, and what kind of gaps exist on the field. For a while the 
AGI field was not so active and the more specific approaches, 'narrow AI', 
grew in popularity. Recently, however, the wider, more general artificial 
intelligence has been regaining interest. This kind of mapping study would 
be needed as the research field is complex and there is no clear presentation 
of the current trends and focal points. Creating this king of overview would 
be a valuable asset for future research, as it would enable focusing the 
research on areas less ventured. Furthermore, if an interesting subtopic 
comes up during the process of mapping, more focus may be directed towards 
that in form of more traditional systematic literature review. This option 
is left for further consideration.

What each chapter is about, how the thesis is structured etc.


\chapter{Artificial General Intelligence} 

TODO: Here some chapter introducing text

\section{History of Artificial Intelligence} 

Even though the idea of autonomous machinery has been around since the ancient 
Greek (\cite{}), AI's origins are set around in the 1940s. At the time, 
American science fiction author Isaac Asimov wrote numerous novels and 
short stories about conscious robots and technology's relation to humankind. 
His work has inspired countless people in the field's of AI and computer science
since then (\cite{kaplan2019}).
Also in the 1940s, mathematician Alan Turing's work on Britain's 
code breaking efforts lead to the creation of first electromechanical computer, 
The Bombe (\cite{kaplan2019}). Turing later gave lectures and wrote an article 
titled \emph{"Computing Machinery and Intelligence"} (\cite*{turing1950}), 
in which he presented several ideas later prevalent in AI field, including the
 "Imitation game", a test to measure the intelligence of a machine 
 (\cite{norvig2002}). This later became well known as the Turing test.

The term Artificial Intelligence was coined in 1956 during a two-month workshop 
\emph{Darthmouth Summer Research Project on Artificial Intelligence}, organized 
by John McCarthy and Marvin Minsky (\cite{kaplan2019}). The participants of the 
workshop would later become the most prominent figures of AI research. During 
DSRPAI two researchers, Allen Newell and Herbert Simon presented Logic Theorist,
 their existing reasoning program, capable of proving multiple mathematical 
 theorems (\cite{norvig2002}). Based on this work the two later created General 
 Problem Solver, GPS, which could solve simple puzzles like Towers of Hanoi 
 using human like recursive approach (\cite{newell1959}). The early days of AI 
 research produced many similar results in different areas. IBM's Arthur Samuel 
 created AI programs that learned to play checkers at a strong amateur level 
 (\cite{norvig2002}).
John McCarthy's 1958 paper titled "Programs with common sense", describes Advice 
Taker, a complete but hypothetical AI system with general knowledge about the
world and deductive processes to manipulate it. The paper is still thought to be
relevant today. McCarthy's system was able to acquire new skills in previously
unknown areas without being reprogrammed.

During these years also work on the neural networks started to gain interest.
The initial work of McCulloch and Pitts (\cite*{mcculloch1943}), later
demonstrated by Hebb (\cite{norvig2002}), showed that a neural network is
capable of learning. In 1960s Rosenblatt's work on perceptrons and Widrow and
Hoff's LMS algorithm were some of the biggest advances in the area
(\cite{widrow1995}). The next great discovery that would propel the neural
networks into the focal point of AI research would happen in the mid-1980s when
the backpropagation algorithm originally presented by Bryson and Ho in 1969 was
rediscovered by multiple independent groups (\cite{norvig2002}). Backpropagation
is one of the most widely used algorithms for training neural networks these
days for its relative power and simplicity (\cite{rumelhart1995}).

History of artificial intelligence contains occasional periods of reduced
interest and funding. These so called "AI winters" are a result of high
expectations collapsing under criticism. First period that can be considered an
AI winter started in the 1970s, and Russell and Norvig (\cite*{norvig2002})
present the following possible reasons for it: Firstly, the early programs knew
nothing about their context, and solved the problems via syntactic
manipulations. This was especially apparent on machine translation projects. As
a language cannot be fully understood without knowing the full context of the
sentences and other nuances of the language, accurate translation proved to be a
difficult task. Failed translation efforts lead to funding cuts in the US.
Second difficulty pointed out by (\cite{norvig2002}) was the sheer complexity of
the target problems. As the early AI programs were focused on simple tasks,
finding a solution by trial and error was possible in practice. But as the
problems became more complex, "combinatorial explosion" issue became more
apparent. The issue was also discussed in British scientist James Lighthill's
report on the state of the AI (\cite*{lighthill1973}). The report is considered
to be one of the main reasons why the British government decided to cut all AI
funding in all but two universities. Lastly, the limitations of the data
structures used in AI field, such as perceptrons, restricted the capabilities of
the solutions. According to Russell and Norvig (\cite*{norvig2002}) this lead to
funding cuts also in the neural network research.

During and after the first AI winter, there was a considerable amount of
research relating to expert systems (\cite{norvig2002}). These systems perform
their tasks in a way similar to human experts in the specific, narrow domain,
relying on a knowledge encoded into a set of rules (\cite{myers1986}). This
style of AI research was inspired by the success of DENDRAL
(\cite{buchanan1968}), a system developed at Stanford by Ed Feigenbaum, Bruce
Buchanan and Joshua Lederberg. DENDRAL's purpose was to use data from mass
spectrometer to infer the structure of a given molecule. MYCIN, developed in the
1970s (\cite{shortliffe1975}), incorporated domain knowledge acquired through
expert interviews, with the uncertainty of medical evaluation taken into account
via certainty factors (\cite{norvig2002}).

Expert systems gained commercial interest, leading to increased research and
adoption in the industry. Government investments in Japan lead to increased
funding in United States and Britain, leading to an AI boom in the 1980s
(\cite{norvig2002}). After the boom, at the end of the 1980s, the second AI
winter arrived. Participation in AI conferences dropped, several of the new AI
companies met their end, as did the AI research divisions in larger hardware and
software companies (\cite{nilsson2009}). The imminent burst of the bubble was
foreseen by several leading researchers, but their warnings didn't have
considerable effect (\cite{nilsson2009}).

According to Russell and Norvig (\cite*{norvig2002}), around this time the AI
field started to adopt the scientific method. This means the earlier ways of
proposing completely new theories based on vague evidence or oversimplified
examples have been replaced by basis on existing theories, repeatable
experiments, and real-world examples. This newly discovered open-mindedness then
lead to a complete new ways of looking at the AI research. AI solutions based on
existing theories, such as speech recognition based on hidden Markov models,
enables the researchers to build on the rigorous mathematical theory behind it
(\cite{norvig2002}). Work of Judea Pearl (\cite*{pearl1988}) and Peter Cheeceman
(\cite*{cheeseman1985}) on the probabilistic reasoning lead to it being accepted
back into the AI field. Later Pearl's Bayesian networks have been used to handle
uncertainty in AI problems. They are graphical models that join probabilistic
information and dependencies to events, enabling inference using probabilistic
methods (\cite{goertzel2007}).

%Large datasets? maybe just mention somewhere else if needed

%TODO: Is the history too much? Feels like it could/should be more topic focused, now it might seem a bit overkill...

%TODO: agents ? 

In the 21st century artificial intelligence research has been steadily growing.
According to (\cite{liu2018}), not only has the amount of publications in the
field been increasing, but also the collaboration between researchers. The study
also deduces that that AI has become more open-minded and popular, as the rate
of self-references is reducing. One reason for the rising popularity on the
field is the success that narrow AI solutions have presented in multitude of
problems. For example, in classical game of Go, program called AlphaGo developed
by Google-owned DeepMind, defeated the world champion Lee Sedol in 2015
(\cite{silver2016}). Due to Go's computationally complex nature this was a
impressive feat previously thought impossible. Later DeepMind developed even
more advanced versions of AlphaGo, called AlphaGo Zero, and generalized
AlphaZero, which could even play Shogi and Chess on superhuman level
(\cite{silver2018}).


Recent years majority of the field has been focusing on the narrow AI approaches
(\cite{goertzel2007}). However, the interest in the classical, strong AI has
also been increasing. This can be seen in the publications from many influential
AI researchers. Authors like John McCarthy (\cite*{mccarthy2007}), Nils Nilsson
(\cite*{nilsson2005}) and Marvin Minsky (\cite*{minsky2007}) have voiced their
opinions that efforts to create a more general AI should be pursued. There are
several terms used regarding these efforts. \textbf{Human-level Artificial
Intelligence} (HLAI) aims to reach "human-level intelligence" and common sense,
a goal that according to Marvin Minsky (\cite*{minsky2004}) can be reached by
not using any single method, but a combination of different resources and
methods. Similar term is \textbf{Artificial General Intelligence} (AGI)
presented by Ben Goertzel and Casio Pennachin (\cite*{goertzel2007}). The goal
of AGI is similar to HLAI, to create an AI system that can express general
intelligence instead of being locked into a single domain. On the next chapter
this general approach is presented in more detail, as it is the focus of this
thesis.


\section{Definition}

% Definition of intelligence?
In order to be able to define AGI, or artificial intelligence in general, one
must first consider the definitions of intelligence in general. The exists many
different definitions, in many different branches of science. Legg and Hutter
(\cite*{legg2007}) list over 60 definitions collected from various academic
sources. These include, for example, \emph{"the general mental ability involved
in calculating, reasoning, perceiving relationships and analogies, learning
quickly, storing and retrieving information,using language fluently,
classifying, generalizing, and adjusting to new situations."} (Columbia
Encyclopedia, sixth edition, 2006), \emph{"that facet of mind underlying our
capacity to think, to solve novel problems, to reason and to have knowledge of
the world"} (\cite{anderson2006}), and \emph{"Intelligence is the ability for an
information processing system to adapt to its environment with insufficient
knowledge and resources."} (\cite{wang1995}). Based on the aforementioned
collection of definitions, Legg and Hutter (\cite*{legg2007}) have formed the
following definition: \emph{"Intelligence measures an agent's ability to achieve
goals in a wide range of environments"}. This gives us a single definition which
encompasses the common traits in intelligence definitions. 

%Intelligence is a trait that manifests itself in multitude of ways, making its
%definition but also its measurement a challenge. 

% gardner, piaget, vygotsky
% Measurement? or later when roadmap is discussed?
% in any case, needs something more here, now just quotes :(

Artificial General Intelligence, sometimes referred as "strong ai", according to
Goertzel and Pennachin (\cite*{goertzel2007}) means \emph{"AI systems that
possess a reasonable degree of self-understanding and autonomous self-control,
and have the ability to solve a variety of complex problems in a variety of
contexts, and to learn to solve new problems that they didn't know about at the
time of their creation."}. It can be seen that an agent fulfilling this
definition also possesses the intelligence defined in the previous chapter. In
this thesis terms artificial general intelligence and human-level artificial
intelligence are treated as synonyms, as they pursue more or less the same goal
of general intelligence. Goertzel and Pennachin (\cite*{goertzel2007}) suggest
that the term AGI is more fitting to the area than HLAI as it human-like
approaches are not necessarily used. 

The reason general intelligence is specified instead of plain intelligence is
that there is a need to differentiate it from the domain specific artificial
intelligence, also known as "narrow AI" or "weak AI", that has become prevalent
in AI research in recent past. Terms strong AI and weak AI were coined by John
Searle in 1980 (\cite{searle1980}). Narrow AI means smart solutions that may
learn and improve their performance through training, but they are only focused
on specific type of problems in a specific context. Examples of such AI include
chess engines, autonomous vehicles, and natural language processing. These
solutions may outperform human capabilities, but only in their limited tasks.
When presented a problem outside their domain, they usually perform poorly. As
the above definition by Goertzel and Pennachin describes, AGI is able to
function on different context and tasks without separate human intervention and
reconfiguration.


%TODO: should this roadmap be placed somewhere else? now seems a bit disconnected
As The AGI community is diverse and there are multitude of opinions on the best
approaches and the goals that should be pursued in the research, several
possible roadmaps have been presented in an attempt to create a common basis for
the discussion and research of human-level artificial general intelligence. In
(\cite{adams2012}) a high level roadmap with AGI's initial required capabilities
and scenario-based milestones is suggested, building on previous work and
workshops organized in 2008 and 2009. Presented scenarios can be used to measure
the progress and capabilities of AGI restricting the progress of different
approaches to a single test situation (\cite{adams2012}). More concrete example
is provided by Ben Goertzel and Gino Yu, who outline creation of a AGI-oriented
cognitive architecture based on existing CogPrime architecture
(\cite{goertzel2014map}). A simultaneous development of multiple AGI-style
applications is suggested to maintain the generality of intelligence. CogPrime
is implemented with OpenCog framework, developed by OpenCog Foundation and AI
researcher Ben Goertzel. OpenCog is an attempt to create an open source
framework for artificial general intelligence
(\cite{opencogwiki,goertzel2012cog}).

TODO: Add something like: One motivation behind this thesis is to find out if
these roadmaps and "common ground" have actually lead to anything concrete, or
has their effort been for nothing.


\chapter{Systematic literature mapping process}


\label{method}

Systematic literature mapping is a secondary study method that helps to identify
the focal points and research gaps in the subject area, providing an overview of
previous research (\cite{petersen2008}). This chapter introduces the mapping
method, describing each phase of the research process. The key differences with
a more popular study method, systematic literature review (SLR) are presented.

\section{Research method description}


Systematic mapping is a common research method used in fields such as evidence
based medicine, but have until recently been rare in software engineering
(\cite{petersen2008}). According to Kitchenham et al. (\cite{kitchenham2010}), 



The systematic literature mapping in this thesis is following the method
guidelines presented by Petersen et al (\cite*{petersen2008}), later updated in
by Petersen, Vakkalanka and Kuzniarz (\cite*{petersen2015}). The mapping process
overview can be seen in figure~\ref{fig:mapping}. It consists of five separate
phases: \textbf{definition of research questions, conducting search, screening of
papers, keywording, and data extraction and mapping.} Each phase produces a
subresult to be used in the next one. This process results in a systematic map
of the area. This can and should be further visualized using for example bubble
graphs (\cite{petersen2008}). Visualization enables easier recognition of 
research gaps and focus points in the target area.


%TODO: SVG picture? now bad quality...
\begin{figure}[h]
  \includegraphics[width=\linewidth]{images/method_graph_L.png}
  \caption{Process model (\cite{petersen2008})}
  \label{fig:mapping}
\end{figure}

\textbf{The following text is very raw, will be edited properly later}

% definition of research questions, 
The process begins by defining focused research questions that are aligned
with the goal of the study. The goal is usually to create a general overview
of the research area, and to identify the type and quantity of research.
Unlike in more focused systematic literature reviews, the research questions
of mapping studies are less focused and cover a broader scope.

% conducting search, 
The next phase is the initial material search, which can be conducted multiple
ways. Search strings can be formulated from the research questions, and used on
academic databases and search engines. For example, databases such as IEEE
Explore and ACM, as well as aggregators like Google Scholar. As the goal is to
achieve broad overview of the research, study outcomes are not taken into
account.
The search can also be conducted manually on specific journals and
conference publications that cover the target area. This is the approach used in
this thesis. This search method also facilitates the initial exclusion of
irrelevant papers, limiting the possible material to the similar fields.

% screening of papers, 
After the initial material has been gathered, it is further refined by excluding
papers not relevant to answering the research questions. Separate criteria for
both inclusion and exclusion is used to find the papers fit for further
analysis. Criteria can be meta-level, such as publication source, year and format,
and content-based, considering the article itself.

% keywording, 
Once the final set of papers is narrowed down and determined, keywording is
performed. First, the research papers' abstracts are analyzed by looking up
possible keywords and common concepts from them. The keywords of each paper are
then combined together. If abstracts' are low quality, the introduction and
conclusion of the paper can also be examined. On some cases inspecting the whole
article might be required.  Different facets can be used, for
example topical and one reflecting the research approach (topic-independent). 
After the final set of keywords is chosen, they are clustered into categories
representing the population.

% data extraction and mapping.
In the final phase of the mapping process, the papers are sorted into the
classification scheme. The schema may evolve during the data extraction process,
changing the categories to match the article population. Once the categorization
is complete, the resulting frequencies of papers can be presented either via
visualization or summary statistics. Visualization using e.g. bubble plots is
preferred, as it is a powerful way to represent the information and map of the
field.


\section{Difference with other secondary studies}

\section{Mapping studies in field of IT}

\chapter{Conducting the literature mapping}

- To ensure study is conducted with rigor, should it be documented and reported
very carefully.

\section{Background/reasoning behind method choice}

\section{Research questions}

The following research questions...

\begin{enumerate}
  \item How much, and what type of research is done in the field of AGI?
  \item Where is the AGI research focused on?
  \item Has there been any major breakthroughs?
  \item Where and when were the studies published?
\end{enumerate}

\section{Sources and databases used}
- Journal listing and their date ranges etc.

- Why journals were chosen, information about them, reliability, area coverage,
jufo rankings etc.

- search terms here or another section? 

- table showing used search phrases?

\section{conducting search}
Search phrases are used on different databases, limiting the papers to amount
possible to handle

Search can be also done by inspecting relevant journals (like here)

- evaluation of search: test set of known papers, possible here?

\section{Criteria for inclusion, exclusion}
Criteria is presented, and papers are further narrowed down.

\section{Keywording}
Papers are further analyzed, keywords are extracted from abstracts and text. 

\section{Data extraction and mapping}
keywords are used to create a mapping, working iteratively. Papers are
categorized based on the emergent mapping.

\section{Source material control}
- How the papers were handled
- How graphs etc. were made
- Other meta level information

\chapter{Results and analysis}
The results of the thesis will be a clear overview of the recent research in the
field of artificial general intelligence in form of classification data, visual
graphs and further synthesis. As mentioned earlier, if an interesting topic
presents itself during the process of mapping, it may be further examined in a
more focused way, such as literature review. This will be considered at that
time.

\section{Validity threats?}

- performed alone, researcher bias etc (multiple people not possible here),
how rigor is achieved, repeatability, 

Good source on reliability \cite{wohlin2013}

\section{Results of literature mapping}
- Graphs and other visualization, bubble graphs are useful.
- Key topics should be described shortly?

\section{Possible continuation research}

- List of most prominent topics for further research

\chapter{Conclusion}
In this thesis, a systematic literature mapping was conducted on the field of
artificial general intelligence. Results of the study showed that .... 

\printbibliography


\end{document}