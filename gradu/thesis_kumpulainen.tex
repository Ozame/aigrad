\documentclass[utf8,english]{gradu3}

\usepackage{graphicx} % for including pictures

\usepackage{amsmath} % useful for math (optional)

\usepackage{booktabs} % good for beautiful tables

\usepackage{csquotes} % ensure proper formatting

\usepackage[authordate,backend=biber,noibid]{biblatex-chicago}

% NOTE: This must be the last \usepackage in the whole document!
\usepackage[bookmarksopen,bookmarksnumbered,linktocpage]{hyperref}

\addbibresource{thesisReferences.bib} % The file name of your bibliography database

\begin{document}

\title{Artificial General Intelligence - a systematic mapping study}
\translatedtitle{Yleistekoäly - systemaattinen kirjallisuuskartoitus}
\studyline{Mathematical Information Technology}
\avainsanat{%
  Pro Gradu,tutkielma, AI, tekoäly}
\keywords{Master's Theses, AGI, AI, artificial intelligence, systematic literature mapping, mapping study}
\tiivistelma{%
  
  Tässä suunnitelmassa käydään läpi pro gradu -tutkielman mahdollista aihetta ja tutkimustapaa. TODO: translate when abstract done
}
\abstract{%
  In this thesis, a systematic mapping study is performed on the field of artificial general intelligence. The goal of the study is to gain insight about the recent developments in the study field. This includes the focus points of the current reserch, possible research gaps, and how the research itself is conducted. TODO: more accurate, proper abstract
}

\author{Samu Kumpulainen}
\contactinformation{\texttt{samu.p.kumpulainen@student.jyu.fi}}
% use a separate \author command for each author, if there is more than one
\supervisor{Vagan Terziyan}
% use a separate \supervisor command for each supervisor, if there
% is more than one

\maketitle

\mainmatter

%Remember to use chapters in the thesis itself
\chapter{Introduction}
The thesis will be a systematic research mapping on the field of Artificial General Intelligence (AGI). The goal of the thesis is to identify the themes and subfields of AGI research in recent years, what is being researched recently, and what kind of gaps exist on the field. For a while the AGI field was not so active and the more specific approaches, 'narrow AI', grew in popularity. Recently, however, the wider, more general artificial intelligence has been regaining interest. This kind of mapping study would be needed as the research field is complex and there is no clear presentation of the current trends and focal points. Creating this king of overview would be a valuable asset for future research, as it would enable focusing the research on areas less ventured. Furthermore, if an interesting subtopic comes up during the process of mapping, more focus may be directed towards that in form of more traditional systematic literature review. This option is left for further consideration.



\chapter{Artificial General Intelligence} 

\section{History of Artificial Intelligence} 

Even though the idea of autonomous machinery has been around since the ancient Greek (\cite{}), AI's origins are set around in the 1940s. At the time, American science fiction author Isaac Asimov wrote numerous novels and short stories about conscious robots and technology's relation to humankind. His work has inspired countless people in the field's of AI and computer science since then (\cite{kaplan2019}).
Also in the 1940s, mathematician Alan Turing's work on Britain's code breaking efforts lead to the creation of first electromechanical computer, The Bombe (\cite{kaplan2019}). Turing later gave lectures and wrote an article titled \emph{"Computing Machinery and Intelligence"} (\cite*{turing1950}), in which he presented several ideas later prevalent in AI field, including the "Imitation game", a test to measure the intelligence of a machine (\cite{norvig2002}). This later became well known as the Turing test.

The term Artificial Intelligence was coined in 1956 during a two-month workshop \emph{Darthmouth Summer Research Project on Artificial Intelligence}, organized by John McCarthy and Marvin Minsky (\cite{kaplan2019}). The participants of the workshop would later become the most prominent figures of AI research. During DSRPAI two researchers, Allen Newell and Herbert Simon presented Logic Theorist, their existing reasoning program, capable of proving multiple mathematical theorems (\cite{norvig2002}). Based on this work the two later created General Problem Solver, GPS, which could solve simple puzzles like Towers of Hanoi using human like recursive approach (\cite{newell1959}). The early days of AI research produced many similar results in different areas. IBM's Arthur Samuel created AI programs that learned to play checkers at a strong amateur level (\cite{norvig2002}).
John McCarthy's 1958 paper titled "Programs with common sense", describes Advice Taker, a complete but hypothetical AI system with general knowledge about the world and deductive processes to manipulate it. The paper is still thought to be relevant today. McCarthy's system was able to acquire new skills in previously unknown areas without being reprogrammed.

During these years also work on the neural networks started to gain interest. The initial work of McCulloch and Pitts (\cite*{mcculloch1943}), later demonstrated by Hebb (\cite{norvig2002}), showed that a neural network is capable of learning. In 1960s Rosenblatt's work on perceptrons and Widrow and Hoff's LMS algorithm were some of the biggest advances in the area (\cite{widrow1995}). The next great discovery that would propel the neural networks into the focal point of AI research would happen in the mid-1980s when the backpropagation algorithm originally presented by Bryson and Ho in 1969 was rediscovered by multiple independent groups (\cite{norvig2002}). Backpropagation is one of the most widely used algorithms for training neural networks these days for its relative power and simplicity (\cite{rumelhart1995}).

History of artificial intelligence contains occasional periods of reduced interest and funding. These so called "AI winters" are a result of high expectations collapsing under criticism. First period that can be considered an AI winter started in the 1970s, and Russell and Norvig (\cite*{norvig2002}) present the following possible reasons for it: Firstly, the early programs knew nothing about their context, and solved the problems via syntactic manipulations. This was especially apparent on machine translation projects. As a language cannot be fully understood without knowing the full context of the sentences and other nuances of the language, accurate translation proved to be a difficult task. Failed translation efforts lead to funding cuts in the US.
Second difficulty pointed out by (\cite{norvig2002}) was the sheer complexity of the target problems. As the early AI programs were focused on simple tasks, finding a solution by trial and error was possible in practice. But as the problems became more complex, "combinatorial explosion" issue became more apparent. The issue was also discussed in British scientist James Lighthill's report on the state of the AI (\cite*{lighthill1973}). The report is considered to be one of the main reasons why the British government decided to cut all AI funding in all but two universities.
Lastly, the limitations of the data structures used in AI field, such as perceptrons, restricted the capabilities of the solutions. According to Russell and Norvig (\cite*{norvig2002}) this lead to funding cuts also in the neural network research.

During and after the first AI winter, there was a considerable amount of research relating to expert systems (\cite{norvig2002}). These systems perform their tasks in a way similar to human experts in the specific, narrow domain, relying on a knowledge encoded into a set of rules (\cite{myers1986}). This style of AI research was inspired by the success of DENDRAL (\cite{buchanan1968}), a system developed at Stanford by Ed Feigenbaum, Bruce Buchanan and Joshua Lederberg. DENDRAL's purpose was to use data from mass spectrometer to infer the structure of a given molecule. MYCIN, developed in the 1970s (\cite{shortliffe1975}), incorporated domain knowledge acquired through expert interviews, with the uncertainty of medical evaluation taken into account via certainty factors (\cite{norvig2002}).

Expert systems gained commercial interest, leading to increased research and adoption in the industry. Government investments in Japan lead to increased funding in United States and Britain, leading to an AI boom in the 1980s (\cite{norvig2002}). After the boom, at the end of the 1980s, the second AI winter arrived. Participation in AI conferences dropped, several of the new AI companies met their end, as did the AI research divisions in larger hardware and software companies (\cite{nilsson2009}). The imminent burst of the bubble was foreseen by several leading researchers, but their warnings didn't have considerable effect (\cite{nilsson2009}).

According to Russell and Norvig (\cite*{norvig2002}), around this time the AI field started to adopt the scientific method. This means the earlier ways of proposing completely new theories based on vague evidence or oversimplified examples have been replaced by basis on existing theories, repeatable experiments, and real-world examples.
This newly discovered open-mindedness then lead to a complete new ways of looking at the AI research. AI solutions based on existing theories, such as speech recognition based on hidden Markov models, enables the researchers to build on the rigorous mathematical theory behind it (\cite{norvig2002}).
Work of Judea Pearl (\cite*{pearl1988}) and Peter Cheeceman (\cite*{cheeseman1985}) on the probabilistic reasoning lead to it being accepted back into the AI field. Later Pearl's Bayesian networks have been used to handle uncertainty in AI problems. They are graphical models that join probabilistic information and dependencies to events, enabling inference using probabilistic methods (\cite{goertzel2007}).

%Large datasets? maybe just mention somewhere else if needed

%TODO: Is the history too much? Feels like it could/should be more topic focused, now it might seem a bit overkill...

%TODO: agents ? 

In the 21st century artificial intelligence research has been steadily growing. According to (\cite{liu2018}), not only has the amount of publications in the field been increasing, but also the collaboration between researchers. The study also deduces that that AI has become more open-minded and popular, as the rate of self-references is reducing. One reason for the rising popularity on the field is the success that narrow AI solutions have presented in multitude of problems. For example, in classical game of Go, program called AlphaGo developed by Google-owned DeepMind, defeated the world champion Lee Sedol in 2015 (\cite{silver2016}). Due to Go's computationally complex nature this was a impressive feat previously thought impossible. Later DeepMind developed even more advanced versions of AlphaGo, called AlphaGo Zero, and generalized AlphaZero, which could even play Shogi and Chess on superhuman level (\cite{silver2018}).


Recent years majority of the field has been focusing on the narrow AI approaches (\cite{goertzel2007}). However, the interest in the classical, strong AI has also been increasing. This can be seen in the publications from many influential AI researchers. Authors like John McCarthy (\cite*{mccarthy2007}), Nils Nilsson (\cite*{nilsson2005}) and Marvin Minsky (\cite*{minsky2007}) have voiced their opinions that efforts to create a more general AI should be pursued. There are several terms used regarding these efforts. \textbf{Human-level Artificial Intelligence} (HLAI) aims to reach "human-level intelligence" and common sense, a goal that according to Marvin Minsky (\cite*{minsky2004}) can be reached by not using any single method, but a combination of different resources and methods.
Similar term is \textbf{Artificial General Intelligence} (AGI) presented by Ben Goertzel and Casio Pennachin (\cite*{goertzel2007}). The goal of AGI is similar to HLAI, to create an AI system that can express general intelligence instead of being locked into a single domain. On the next chapter this general approach is presented in more detail, as it is the focus of this thesis.


\section{Definition}

% Definition of intelligence?
In order to be able to define AGI, or artificial intelligence in general, one must first consider the definitions of intelligence in general. The exists many different definitions, in many different branches of science. Legg and Hutter (\cite*{legg2007}) list over 60 definitions collected from various academic sources. These include, for example, \emph{"the general mental ability involved in calculating, reasoning, perceiving relationships and analogies, learning quickly, storing and retrieving information,using language fluently, classifying, generalizing, and adjusting to new situations."} (Columbia Encyclopedia, sixth edition, 2006), \emph{"that facet of mind underlying our capacity to think, to solve novel problems, to reason and to have knowledge of the world"} (\cite{anderson2006}), and \emph{"Intelligence is the ability for an information processing system to adapt to its environment with insufficient knowledge and resources."} (\cite{wang1995}).
Based on the aforementioned collection of definitions, Legg and Hutter (\cite*{legg2007}) have formed the following definition: \emph{"Intelligence measures an agent's ability to achieve goals in a wide range of environments"}. This gives us a single definition which encompasses the common traits in intelligence definitions. 

%Intelligence is a trait that manifests itself in multitude of ways, making its definition but also its measurement a challenge. 
% gardner, piaget, vygotsky
% Measurement? or later when roadmap is discussed?
% in any case, needs something more here, now just quotes :(

Artificial General Intelligence, sometimes referred as "strong ai", according to Goertzel and Pennachin (\cite*{goertzel2007}) means \emph{"AI systems that possess a reasonable degree of self-understanding and autonomous self-control, and have the ability to solve a variety of complex problems in a variety of contexts, and to learn to solve new problems that they didn't know about at the time of their creation."}. It can be seen that an agent fulfilling this definition also possesses the intelligence defined in the previous chapter. In this thesis terms artificial general intelligence and human-level artificial intelligence are treated as synonyms, as they pursue more or less the same goal of general intelligence. Goertzel and Pennachin (\cite*{goertzel2007}) suggest that the term AGI is more fitting to the area than HLAI as it human-like approaches are not necessarily used. 

 The reason general intelligence is specified instead of plain intelligence is that there is a need to differentiate it from the domain specific artificial intelligence, also known as "narrow AI" or "weak AI", that has become prevalent in AI research in recent past. Terms strong AI and weak AI were coined by John Searle in 1980 (\cite{searle1980}). Narrow AI means smart solutions that may learn and improve their performance through training, but they are only focused on specific type of problems in a specific context. Examples of such AI include chess engines, autonomous vehicles, and natural language processing. These solutions may outperform human capabilities, but only in their limited tasks. When presented a problem outside their domain, they usually perform poorly.
As the above definition by Goertzel and Pennachin describes, AGI is able to function on different context and tasks without separate human intervention and reconfiguration.


%TODO: should this roadmap be placed somewhere else? now seems a bit disconnected
As The AGI community is diverse and there are multitude of opinions on the best approaches and the goals that should be pursued in the research, several possible roadmaps have been presented in an attempt to create a common basis for the discussion and research of human-level artificial general intelligence. In (\cite{adams2012}) a high level roadmap with AGI's initial required capabilities and scenario-based milestones is suggested, building on previous work and workshops organized in 2008 and 2009. Presented scenarios can be used to measure the progress and capabilities of AGI restricting the progress of different approaches to a single test situation (\cite{adams2012}).
More concrete example is provided by Ben Goertzel and Gino Yu, who outline creation of a AGI-oriented cognitive architecture based on existing CogPrime architecture (\cite{goertzel2014map}). A simultaneous development of multiple AGI-style applications is suggested to maintain the generality of intelligence. CogPrime is implemented with OpenCog framework, developed by OpenCog Foundation and AI researcher Ben Goertzel. OpenCog is an attempt to create an open source framework for artificial general intelligence (\cite{opencogwiki,goertzel2012cog}).

TODO: Add something like: One motivation behind this thesis is to find out if these roadmaps and "common ground" have actually lead to anything concrete, or has their effort been for nothing.


\chapter{Systematic literature mapping process}


\label{method}

Systematic literature mapping is a secondary study method that helps to identify the focal points and research gaps in the subject area, providing an overview of previous research (\cite{petersen2008}). This chapter introduces the mapping method, describing each phase of the process. The key differences with a more popular study method, systematic literature review are presented.

\section{Research method and reasoning}

The systematic literature mapping in this thesis is following the method guidelines presented by Petersen et al (\cite{petersen2008}), later updated in (\cite{petersen2015}).

%TODO: SVG picture? now bad quality...

\begin{figure}
  \includegraphics[width=\linewidth]{images/method_graph_L.png}
  \caption{Process model \cite{petersen2008}}
  \label{fig:mapping}
\end{figure}



According to Peterson et al. (\cite*{petersen2008}), it consist of the following phases:
\begin{enumerate}
    \item Definition of research questions
    \item Conducting search
    \item Screening the papers for inclusion and exclusion
    \item Keywording (abstracts, conclusions, introductions)
    \item Data extraction and mapping
\end{enumerate}

Each phase produces a subresult to be used in the next one. This process results in a systematic map of the area. This can and should be further visualized using for example bubble graphs (\cite{mononen2018} and \cite{petersen2008}). This helps to more easily spot the focus points and gaps in the research.


The material is to be gathered through databases and search engines via specified search terms. Databases and content libraries such as IEEE, ACM, and Google Scholar can be used. One possible option would be to focus on journals that specialize on the field, such as \textit{Journal of Artificial General Intelligence, Journal of Artificial Intelligence Research}, and \textit{Artificial Intelligence}, the first of which is highly focused on the area, but not well ranked based by Publication Forum. 

There exists many good papers on the research method. There are some example theses using the approach, such as Niko Mononen's (\cite{mononen2018}) and Sari Ryynänen's (\cite{ryynanen2017}). Guidelines regarding the method itself can be found on article ny Petersen et al. \cite*{petersen2008}. There is an update on the topic as well (\cite{petersen2015}). Some sources about literature reviews such as Bereton et al. (\cite*{brereton2007}) and Salminen (\cite*{salminen2011}) might be useful as well.

\section{Difference with other secondary studies}

\section{Mapping studies in field of IT}

\chapter{The literature mapping}

\section{Background/why this method and topic}

\section{Research questions}

The following research questions...

\begin{enumerate}
  \item How much, and what type of research is done in the field of AGI?
  \item Where is the AGI research focused on?
  \item Has there been any major breakthroughs?
  \item Where and when were the studies published?
\end{enumerate}

\section{Sources and databases used}
- Journal listing and their date ranges etc.

- search terms here or another section? 

- table showing used search phrases?

\section{conducting search}
search phrases are used on different databases, limiting the papers to amount possible to handle

\section{Criteria for inclusion, exclusion}
papers from the 

\section{Keywording}
papers are further analyzed, keywords are extracted from abstracts, 

\section{Data extraction and mapping}
keywords are mapped using frequencies etc

\section{Source material control}
- How the papers were handled
- How graphs etc. were made

\chapter{Results and analysis}
The results of the thesis will be a clear overview of the recent research in the field of artificial general intelligence in form of classification data, visual graphs and further synthesis. As mentioned earlier, if an interesting topic presents itself during the process of mapping, it may be further examined in a more focused way, such as literature review. This will be considered at that time.

\section{Results of literature mapping}
- Graphs and other visualization, bubble graphs are useful.

\section{Possible continuation research}

- List of most prominent topics for further research

\chapter{Conclusion}
In this thesis, a systematic literature mapping was conducted on the field of artificial general intelligence. Results of the study showed that .... 

\printbibliography


\end{document}